{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pix2PixHD Next Frame Prediction.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yjawahar/art/blob/main/Pix2PixHD_Next_Frame_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXoBaXRDKuMV"
      },
      "source": [
        "#Next Frame Prediction using Pix2PixHD\n",
        "\n",
        "By Derrick Schultz\n",
        "\n",
        "Forked repo and tutorial based on [JC Testud’s excellent repo and Medium](https://medium.com/@jctestud/video-generation-with-pix2pix-aed5b1b69f57) article."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfOexYWJX3Pt"
      },
      "source": [
        "## Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K--AsrIzpH58",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81afd7a2-dcd3-477a-d617-8ef04654f6d8"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9D_7iUQINoa"
      },
      "source": [
        "Check to see what GPU we’re assigned"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABG3hL4lIQGP",
        "outputId": "77460c4d-80be-487b-eb9e-04050b96e8e0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!nvidia-smi -L"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU 0: Tesla T4 (UUID: GPU-4e777289-4636-425c-467e-2830358dfc48)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUvLbCJtLqaV"
      },
      "source": [
        "## Install libraries and dependencies\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQbRsmWdvjUo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44a4cad7-3ad1-4f9e-e587-72b159e507ec"
      },
      "source": [
        "import os\n",
        "if os.path.isdir(\"/content/drive/MyDrive/nfp-colab/pix2pixHD/\"):\n",
        "    %cd /content/drive/MyDrive/nfp-colab/pix2pixHD/\n",
        "    # !git pull\n",
        "    !pip install dominate\n",
        "else:\n",
        "    %cd /content/drive/MyDrive\n",
        "    !mkdir nfp-colab\n",
        "    %cd nfp-colab\n",
        "    !git clone -b video https://github.com/dvschultz/pix2pixHD.git\n",
        "    !pip install dominate\n",
        "    %cd pix2pixHD/"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/nfp-colab/pix2pixHD\n",
            "Collecting dominate\n",
            "  Downloading dominate-2.9.1-py2.py3-none-any.whl.metadata (13 kB)\n",
            "Downloading dominate-2.9.1-py2.py3-none-any.whl (29 kB)\n",
            "Installing collected packages: dominate\n",
            "Successfully installed dominate-2.9.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzLGf05WXMFV"
      },
      "source": [
        "## Extract frames from video\n",
        "\n",
        "Upload a video to either Colab or Google Drive.\n",
        "\n",
        "* `-video` is the path to the video file\n",
        "* `-name` should be a unique name (think of it like a project name)\n",
        "* `-width` and `-height` **must** to be a multiple of 32\n",
        "* `-p2pdir` leave as `.` unless you know it shouldn’t be that ;)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWRL2ty6N9LD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11c0a4b0-ba5a-4b5b-eb32-6f8d567429b9"
      },
      "source": [
        "!python extract_frames.py -video /content/cuttlefish.mp4 -name cuttlefish -p2pdir . -width 1280 -height 736"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "creating the dataset structure\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/nfp-colab/pix2pixHD/extract_frames.py\", line 29, in <module>\n",
            "    os.mkdir(dataset_dir)\n",
            "FileExistsError: [Errno 17] File exists: '/content/drive/MyDrive/nfp-colab/pix2pixHD/datasets/cuttlefish'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qL9BZkBA_QRR"
      },
      "source": [
        "## Train your model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4X7qahzMX05u"
      },
      "source": [
        "### Initial training\n",
        "\n",
        "Note: if you have a large dataset, this may timeout initially (`ValueError: __len__() should return >= 0`). Give it a minute or two and run it again.\n",
        "\n",
        "*   `--name` should be a unique name (think of it like a project name). For your sanity I recommend using the same `-name` as above.\n",
        "*   `--dataroot` should point to your dataset. This will usually be in `./datasets/[your project name]`\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzHBGVnUKEzE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27ed25be-be67-45da-e3fc-b13744f24002"
      },
      "source": [
        "!python train_video.py --name cuttlefish --dataroot ./datasets/cuttlefish/ --save_epoch_freq 5 #--continue_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------ Options -------------\n",
            "batchSize: 1\n",
            "beta1: 0.5\n",
            "checkpoints_dir: ./checkpoints\n",
            "continue_train: False\n",
            "data_type: 32\n",
            "dataroot: ./datasets/cuttlefish/\n",
            "debug: False\n",
            "display_freq: 100\n",
            "display_winsize: 512\n",
            "feat_num: 3\n",
            "fineSize: 512\n",
            "fp16: False\n",
            "fps: 24.0\n",
            "gpu_ids: [0]\n",
            "heat_seeking_lvl: 0\n",
            "input_nc: 3\n",
            "instance_feat: False\n",
            "isTrain: True\n",
            "label_feat: False\n",
            "label_nc: 35\n",
            "lambda_feat: 10.0\n",
            "loadSize: 1024\n",
            "load_features: False\n",
            "load_pretrain: \n",
            "local_rank: 0\n",
            "lr: 0.0002\n",
            "max_dataset_size: inf\n",
            "model: pix2pixHD\n",
            "nThreads: 2\n",
            "n_blocks_global: 9\n",
            "n_blocks_local: 3\n",
            "n_clusters: 10\n",
            "n_downsample_E: 4\n",
            "n_downsample_global: 4\n",
            "n_layers_D: 3\n",
            "n_local_enhancers: 1\n",
            "name: cuttlefish\n",
            "ndf: 64\n",
            "nef: 16\n",
            "netG: global\n",
            "ngf: 64\n",
            "niter: 100\n",
            "niter_decay: 100\n",
            "niter_fix_global: 0\n",
            "no_flip: False\n",
            "no_ganFeat_loss: False\n",
            "no_html: False\n",
            "no_instance: False\n",
            "no_lsgan: False\n",
            "no_vgg_loss: False\n",
            "norm: instance\n",
            "num_D: 2\n",
            "output_nc: 3\n",
            "phase: train\n",
            "pool_size: 0\n",
            "print_freq: 100\n",
            "pstart: 1\n",
            "pstop: 1\n",
            "resize_or_crop: scale_width\n",
            "save_epoch_freq: 5\n",
            "save_latest_freq: 1000\n",
            "scheduled_sampling: False\n",
            "serial_batches: False\n",
            "ss_recursion_prob: 0.2\n",
            "start_frames_before: 1\n",
            "start_from: video\n",
            "tf_log: False\n",
            "use_dropout: False\n",
            "verbose: False\n",
            "video_mode: False\n",
            "which_epoch: latest\n",
            "zoom_cres: False\n",
            "zoom_inc: 24\n",
            "zoom_lvl: 0\n",
            "-------------- End ----------------\n",
            "CustomDatasetDataLoader\n",
            "dataset [FrameDataset] was created\n",
            "FrameDataset initialized from: ./datasets/cuttlefish/train_frames\n",
            "contains 348 frames, 347 consecutive pairs\n",
            "#training images = 347\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n",
            "100% 548M/548M [00:05<00:00, 107MB/s] \n",
            "create web directory ./checkpoints/cuttlefish/web...\n",
            "/content/drive/MyDrive/nfp-colab/pix2pixHD/models/networks.py:97: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:78.)\n",
            "  fake_tensor = self.Tensor(input.size()).fill_(self.fake_label)\n",
            "(epoch: 1, iters: 100, time: 2.231) G_GAN: 2.182 G_GAN_Feat: 8.750 G_VGG: 4.602 D_real: 1.306 D_fake: 1.337 \n",
            "(epoch: 1, iters: 200, time: 2.281) G_GAN: 1.140 G_GAN_Feat: 6.754 G_VGG: 3.683 D_real: 0.301 D_fake: 0.289 \n",
            "(epoch: 1, iters: 300, time: 2.278) G_GAN: 1.369 G_GAN_Feat: 6.958 G_VGG: 3.184 D_real: 0.624 D_fake: 0.480 \n",
            "End of epoch 1 / 200 \t Time Taken: 785 sec\n",
            "(epoch: 2, iters: 53, time: 2.280) G_GAN: 0.940 G_GAN_Feat: 6.402 G_VGG: 5.310 D_real: 0.398 D_fake: 0.438 \n",
            "(epoch: 2, iters: 153, time: 2.278) G_GAN: 0.960 G_GAN_Feat: 7.917 G_VGG: 6.328 D_real: 0.550 D_fake: 0.431 \n",
            "(epoch: 2, iters: 253, time: 2.277) G_GAN: 1.951 G_GAN_Feat: 7.482 G_VGG: 3.218 D_real: 0.558 D_fake: 0.246 \n",
            "End of epoch 2 / 200 \t Time Taken: 790 sec\n",
            "(epoch: 3, iters: 6, time: 2.277) G_GAN: 1.537 G_GAN_Feat: 7.475 G_VGG: 2.371 D_real: 0.334 D_fake: 0.524 \n",
            "(epoch: 3, iters: 106, time: 2.279) G_GAN: 1.148 G_GAN_Feat: 5.274 G_VGG: 2.049 D_real: 0.196 D_fake: 0.282 \n",
            "(epoch: 3, iters: 206, time: 2.280) G_GAN: 1.779 G_GAN_Feat: 5.876 G_VGG: 2.005 D_real: 0.213 D_fake: 0.047 \n",
            "(epoch: 3, iters: 306, time: 2.281) G_GAN: 1.188 G_GAN_Feat: 5.625 G_VGG: 2.134 D_real: 0.282 D_fake: 0.344 \n",
            "saving the latest model (epoch 3, total_steps 1000)\n",
            "End of epoch 3 / 200 \t Time Taken: 797 sec\n",
            "(epoch: 4, iters: 59, time: 2.282) G_GAN: 0.845 G_GAN_Feat: 8.343 G_VGG: 4.691 D_real: 0.228 D_fake: 0.529 \n",
            "(epoch: 4, iters: 159, time: 2.279) G_GAN: 1.229 G_GAN_Feat: 11.312 G_VGG: 5.452 D_real: 0.499 D_fake: 0.187 \n",
            "(epoch: 4, iters: 259, time: 2.281) G_GAN: 0.612 G_GAN_Feat: 5.563 G_VGG: 2.000 D_real: 0.221 D_fake: 0.567 \n",
            "End of epoch 4 / 200 \t Time Taken: 791 sec\n",
            "(epoch: 5, iters: 12, time: 2.279) G_GAN: 1.840 G_GAN_Feat: 7.078 G_VGG: 1.770 D_real: 0.243 D_fake: 0.121 \n",
            "(epoch: 5, iters: 112, time: 2.281) G_GAN: 1.901 G_GAN_Feat: 10.760 G_VGG: 5.144 D_real: 0.195 D_fake: 0.115 \n",
            "(epoch: 5, iters: 212, time: 2.280) G_GAN: 0.785 G_GAN_Feat: 7.706 G_VGG: 4.154 D_real: 0.101 D_fake: 0.371 \n",
            "(epoch: 5, iters: 312, time: 2.281) G_GAN: 0.883 G_GAN_Feat: 9.653 G_VGG: 5.083 D_real: 0.228 D_fake: 0.683 \n",
            "End of epoch 5 / 200 \t Time Taken: 791 sec\n",
            "saving the model at the end of epoch 5, iters 1735\n",
            "(epoch: 6, iters: 65, time: 2.374) G_GAN: 0.639 G_GAN_Feat: 5.808 G_VGG: 1.687 D_real: 0.134 D_fake: 0.535 \n",
            "(epoch: 6, iters: 165, time: 2.281) G_GAN: 1.450 G_GAN_Feat: 10.724 G_VGG: 5.285 D_real: 0.195 D_fake: 0.129 \n",
            "(epoch: 6, iters: 265, time: 2.282) G_GAN: 0.915 G_GAN_Feat: 7.618 G_VGG: 4.632 D_real: 0.666 D_fake: 0.391 \n",
            "saving the latest model (epoch 6, total_steps 2000)\n",
            "End of epoch 6 / 200 \t Time Taken: 795 sec\n",
            "(epoch: 7, iters: 18, time: 2.283) G_GAN: 0.602 G_GAN_Feat: 4.583 G_VGG: 1.499 D_real: 0.055 D_fake: 0.468 \n",
            "(epoch: 7, iters: 118, time: 2.280) G_GAN: 2.466 G_GAN_Feat: 8.961 G_VGG: 1.813 D_real: 0.280 D_fake: 0.067 \n",
            "(epoch: 7, iters: 218, time: 2.279) G_GAN: 1.896 G_GAN_Feat: 6.525 G_VGG: 3.874 D_real: 0.109 D_fake: 0.085 \n",
            "(epoch: 7, iters: 318, time: 2.277) G_GAN: 1.726 G_GAN_Feat: 5.407 G_VGG: 1.443 D_real: 0.400 D_fake: 0.164 \n",
            "End of epoch 7 / 200 \t Time Taken: 790 sec\n",
            "(epoch: 8, iters: 71, time: 2.278) G_GAN: 1.373 G_GAN_Feat: 5.997 G_VGG: 1.517 D_real: 0.058 D_fake: 0.147 \n",
            "(epoch: 8, iters: 171, time: 2.277) G_GAN: 1.804 G_GAN_Feat: 8.406 G_VGG: 4.181 D_real: 0.348 D_fake: 0.035 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GDg3CeW_1TD"
      },
      "source": [
        "### Continue Training\n",
        "To pick up from a previous training session run the same command you ran to start with and append `--continue_train` to the end of the command."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5q3dE9S_5eg",
        "outputId": "0336dffa-a08c-4613-8947-96932f57c473",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!python train_video.py --name cuttlefish --dataroot ./datasets/cuttlefish/ --save_epoch_freq 1 --continue_train"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------ Options -------------\n",
            "batchSize: 1\n",
            "beta1: 0.5\n",
            "checkpoints_dir: ./checkpoints\n",
            "continue_train: True\n",
            "data_type: 32\n",
            "dataroot: ./datasets/cuttlefish/\n",
            "debug: False\n",
            "display_freq: 100\n",
            "display_winsize: 512\n",
            "feat_num: 3\n",
            "fineSize: 512\n",
            "fp16: False\n",
            "fps: 24.0\n",
            "gpu_ids: [0]\n",
            "heat_seeking_lvl: 0\n",
            "input_nc: 3\n",
            "instance_feat: False\n",
            "isTrain: True\n",
            "label_feat: False\n",
            "label_nc: 35\n",
            "lambda_feat: 10.0\n",
            "loadSize: 1024\n",
            "load_features: False\n",
            "load_pretrain: \n",
            "local_rank: 0\n",
            "lr: 0.0002\n",
            "max_dataset_size: inf\n",
            "model: pix2pixHD\n",
            "nThreads: 2\n",
            "n_blocks_global: 9\n",
            "n_blocks_local: 3\n",
            "n_clusters: 10\n",
            "n_downsample_E: 4\n",
            "n_downsample_global: 4\n",
            "n_layers_D: 3\n",
            "n_local_enhancers: 1\n",
            "name: cuttlefish\n",
            "ndf: 64\n",
            "nef: 16\n",
            "netG: global\n",
            "ngf: 64\n",
            "niter: 100\n",
            "niter_decay: 100\n",
            "niter_fix_global: 0\n",
            "no_flip: False\n",
            "no_ganFeat_loss: False\n",
            "no_html: False\n",
            "no_instance: False\n",
            "no_lsgan: False\n",
            "no_vgg_loss: False\n",
            "norm: instance\n",
            "num_D: 2\n",
            "output_nc: 3\n",
            "phase: train\n",
            "pool_size: 0\n",
            "print_freq: 100\n",
            "pstart: 1\n",
            "pstop: 1\n",
            "resize_or_crop: scale_width\n",
            "save_epoch_freq: 1\n",
            "save_latest_freq: 1000\n",
            "scheduled_sampling: False\n",
            "serial_batches: False\n",
            "ss_recursion_prob: 0.2\n",
            "start_frames_before: 1\n",
            "start_from: video\n",
            "tf_log: False\n",
            "use_dropout: False\n",
            "verbose: False\n",
            "video_mode: False\n",
            "which_epoch: latest\n",
            "zoom_cres: False\n",
            "zoom_inc: 24\n",
            "zoom_lvl: 0\n",
            "-------------- End ----------------\n",
            "Resuming from epoch 34 at iteration 0\n",
            "CustomDatasetDataLoader\n",
            "dataset [FrameDataset] was created\n",
            "FrameDataset initialized from: ./datasets/cuttlefish/train_frames\n",
            "contains 348 frames, 347 consecutive pairs\n",
            "#training images = 347\n",
            "\n",
            "/content/drive/MyDrive/nfp-colab/pix2pixHD/models/base_model.py:64: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  network.load_state_dict(torch.load(save_path))\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n",
            "100% 548M/548M [00:10<00:00, 57.4MB/s]\n",
            "create web directory ./checkpoints/cuttlefish/web...\n",
            "/content/drive/MyDrive/nfp-colab/pix2pixHD/models/networks.py:97: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:78.)\n",
            "  fake_tensor = self.Tensor(input.size()).fill_(self.fake_label)\n",
            "(epoch: 34, iters: 100, time: 2.238) G_GAN: 1.149 G_GAN_Feat: 4.220 G_VGG: 1.028 D_real: 0.087 D_fake: 0.147 \n",
            "(epoch: 34, iters: 200, time: 2.259) G_GAN: 1.248 G_GAN_Feat: 6.685 G_VGG: 1.154 D_real: 0.059 D_fake: 0.293 \n",
            "(epoch: 34, iters: 300, time: 2.259) G_GAN: 0.955 G_GAN_Feat: 8.222 G_VGG: 1.223 D_real: 0.258 D_fake: 0.272 \n",
            "End of epoch 34 / 200 \t Time Taken: 781 sec\n",
            "saving the model at the end of epoch 34, iters 11798\n",
            "(epoch: 35, iters: 53, time: 2.379) G_GAN: 2.035 G_GAN_Feat: 7.978 G_VGG: 3.690 D_real: 0.250 D_fake: 0.082 \n",
            "(epoch: 35, iters: 153, time: 2.257) G_GAN: 1.758 G_GAN_Feat: 7.997 G_VGG: 3.114 D_real: 0.111 D_fake: 0.129 \n",
            "(epoch: 35, iters: 253, time: 2.257) G_GAN: 0.677 G_GAN_Feat: 4.729 G_VGG: 1.196 D_real: 0.047 D_fake: 0.413 \n",
            "End of epoch 35 / 200 \t Time Taken: 783 sec\n",
            "saving the model at the end of epoch 35, iters 12145\n",
            "(epoch: 36, iters: 6, time: 2.380) G_GAN: 2.779 G_GAN_Feat: 5.676 G_VGG: 1.174 D_real: 0.374 D_fake: 0.112 \n",
            "(epoch: 36, iters: 106, time: 2.264) G_GAN: 2.582 G_GAN_Feat: 5.615 G_VGG: 1.029 D_real: 0.090 D_fake: 0.104 \n",
            "(epoch: 36, iters: 206, time: 2.258) G_GAN: 1.049 G_GAN_Feat: 4.489 G_VGG: 1.037 D_real: 0.280 D_fake: 0.245 \n",
            "(epoch: 36, iters: 306, time: 2.256) G_GAN: 1.804 G_GAN_Feat: 8.339 G_VGG: 3.261 D_real: 0.302 D_fake: 0.161 \n",
            "saving the latest model (epoch 36, total_steps 12451)\n",
            "End of epoch 36 / 200 \t Time Taken: 789 sec\n",
            "saving the model at the end of epoch 36, iters 12492\n",
            "(epoch: 37, iters: 59, time: 2.406) G_GAN: 0.913 G_GAN_Feat: 10.213 G_VGG: 4.653 D_real: 0.199 D_fake: 0.383 \n",
            "(epoch: 37, iters: 159, time: 2.259) G_GAN: 0.944 G_GAN_Feat: 9.881 G_VGG: 4.426 D_real: 0.114 D_fake: 0.257 \n",
            "(epoch: 37, iters: 259, time: 2.256) G_GAN: 1.510 G_GAN_Feat: 5.535 G_VGG: 1.110 D_real: 0.387 D_fake: 0.217 \n",
            "End of epoch 37 / 200 \t Time Taken: 782 sec\n",
            "saving the model at the end of epoch 37, iters 12839\n",
            "(epoch: 38, iters: 12, time: 2.354) G_GAN: 1.768 G_GAN_Feat: 6.195 G_VGG: 1.296 D_real: 0.700 D_fake: 0.041 \n",
            "(epoch: 38, iters: 112, time: 2.261) G_GAN: 2.409 G_GAN_Feat: 5.790 G_VGG: 1.025 D_real: 0.057 D_fake: 0.070 \n",
            "(epoch: 38, iters: 212, time: 2.258) G_GAN: 0.598 G_GAN_Feat: 7.324 G_VGG: 1.125 D_real: 0.096 D_fake: 0.542 \n",
            "(epoch: 38, iters: 312, time: 2.260) G_GAN: 3.043 G_GAN_Feat: 5.342 G_VGG: 1.282 D_real: 0.258 D_fake: 0.145 \n",
            "End of epoch 38 / 200 \t Time Taken: 782 sec\n",
            "saving the model at the end of epoch 38, iters 13186\n",
            "(epoch: 39, iters: 65, time: 2.402) G_GAN: 1.050 G_GAN_Feat: 8.235 G_VGG: 3.607 D_real: 0.067 D_fake: 0.242 \n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f30cd8e5080>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1568, in _shutdown_workers\n",
            "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
            "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 149, in join\n",
            "    res = self._popen.wait(timeout)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/multiprocessing/popen_fork.py\", line 40, in wait\n",
            "    if not wait([self.sentinel], timeout):\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/multiprocessing/connection.py\", line 948, in wait\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jly_3OyBoGg2"
      },
      "source": [
        "#Generating Videos\n",
        "\n",
        "To generate a video from your completed model, run the `generate_video.py` script. I recommend keeping the `--name` and `--dataroot` arguments the same.\n",
        "\n",
        "There are a number of additional arguments you’ll need to include in this command:\n",
        "\n",
        "\n",
        "*   `--fps` frame rate for your video\n",
        "*   `--how_many` how many frames you want to create (this + fps = video length)\n",
        "*   `--which_epoch` which epoch you want to generate videos from (note: if you choose `133` but there’s no epoch 133 model file, this will throw an error)\n",
        "*   `--start_from` by default your video will start predicting images from the 60th frame of your training set. You can pass in the path to a different frame to have it start from that frame\n",
        "\n",
        "I recommend playing with both the `--which_epoch` and `--start_from` arguments as you can get dramatically different results by changing in the inputs here.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INVUtG-pt_F6",
        "outputId": "51e45beb-d5dc-4550-a366-343a85503ac3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!python generate_video.py --name cuttlefish --dataroot ./datasets/cuttlefish/ --fps 24 --how_many 600 --which_epoch latest"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------ Options -------------\n",
            "aspect_ratio: 1.0\n",
            "batchSize: 1\n",
            "checkpoints_dir: ./checkpoints\n",
            "cluster_path: features_clustered_010.npy\n",
            "data_type: 32\n",
            "dataroot: ./datasets/cuttlefish/\n",
            "display_winsize: 512\n",
            "engine: None\n",
            "export_onnx: None\n",
            "feat_num: 3\n",
            "fineSize: 512\n",
            "fp16: False\n",
            "fps: 24.0\n",
            "gpu_ids: [0]\n",
            "heat_seeking_lvl: 0\n",
            "how_many: 600\n",
            "input_nc: 3\n",
            "instance_feat: False\n",
            "isTrain: False\n",
            "label_feat: False\n",
            "label_nc: 35\n",
            "loadSize: 1024\n",
            "load_features: False\n",
            "local_rank: 0\n",
            "max_dataset_size: inf\n",
            "model: pix2pixHD\n",
            "nThreads: 2\n",
            "n_blocks_global: 9\n",
            "n_blocks_local: 3\n",
            "n_clusters: 10\n",
            "n_downsample_E: 4\n",
            "n_downsample_global: 4\n",
            "n_local_enhancers: 1\n",
            "name: cuttlefish\n",
            "nef: 16\n",
            "netG: global\n",
            "ngf: 64\n",
            "niter_fix_global: 0\n",
            "no_crop: True\n",
            "no_flip: False\n",
            "no_instance: False\n",
            "norm: instance\n",
            "ntest: inf\n",
            "onnx: None\n",
            "output_nc: 3\n",
            "phase: test\n",
            "png: False\n",
            "pstart: 1\n",
            "pstop: 1\n",
            "resize_or_crop: scale_width\n",
            "results_dir: ./results/\n",
            "scheduled_sampling: False\n",
            "serial_batches: False\n",
            "ss_recursion_prob: 0.2\n",
            "start_frames_before: 1\n",
            "start_from: video\n",
            "tf_log: False\n",
            "use_dropout: False\n",
            "use_encoded_image: False\n",
            "verbose: False\n",
            "video_mode: False\n",
            "which_epoch: latest\n",
            "zoom_cres: False\n",
            "zoom_inc: 24\n",
            "zoom_lvl: 0\n",
            "-------------- End ----------------\n",
            "CustomDatasetDataLoader\n",
            "dataset [FrameDataset] was created\n",
            "FrameDataset initialized from: ./datasets/cuttlefish/test_frames\n",
            "contains 60 frames, 59 consecutive pairs\n",
            "\n",
            "/content/drive/MyDrive/nfp-colab/pix2pixHD/models/base_model.py:64: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  network.load_state_dict(torch.load(save_path))\n",
            "100% 600/600 [04:34<00:00,  2.19it/s]\n",
            "ffmpeg -v 16 -framerate 24 -i ./checkpoints/cuttlefish/frames/frame-%05d.jpg -q:v 2 -filter:v \"crop=1280:720:0:16\" ./checkpoints/cuttlefish/output/epoch-latest_cuttlefish_25.0-s_24.0-fps.mp4\n",
            "building video from frames\n",
            "video ready:\n",
            "./checkpoints/cuttlefish/output/epoch-latest_cuttlefish_25.0-s_24.0-fps.mp4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UiaLMDmdDG3t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55df357a-ca62-479c-cc72-b88ae05b88df"
      },
      "source": [
        "import os\n",
        "\n",
        "def processFolder(folder, epoch = 10, frameCount = 240, skipCount = 1):\n",
        "  files = os.listdir(folder)\n",
        "\n",
        "  count = 0\n",
        "  for f in files:\n",
        "\n",
        "    if (count % skipCount == 0):\n",
        "      print(f)\n",
        "      if epoch == 'latest':\n",
        "        command = 'python generate_video.py --name glitch_white_circle --dataroot /content/drive/MyDrive/nfp-colab/pix2pixHD/datasets/cuttlefish/ --fps 60 --how_many %i --which_epoch latest --start_from %s/%s' % ( frameCount, folder, f)\n",
        "      else:\n",
        "        command = 'python generate_video.py --name glitch_white_circle --dataroot /content/drive/MyDrive/nfp-colab/pix2pixHD/datasets/cuttlefish/ --fps 24 --how_many %i --which_epoch %i --start_from %s/%s' % ( frameCount, epoch, folder, f)\n",
        "      os.system(command)\n",
        "    count += 1\n",
        "\n",
        "processFolder('/content/drive/MyDrive/nfp-colab/pix2pixHD/datasets/cuttlefish/train_frames',1,600,3050)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "frame-000001.jpg\n"
          ]
        }
      ]
    }
  ]
}